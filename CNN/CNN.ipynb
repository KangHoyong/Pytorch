{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "필요한 라이브러리 임포트  \n",
    "- torch.util.data.DataLoader: 데이터를 하나씩 전달하지 않고 원하는 batch size대로 묶어서 전달하거나 더 효율적인 학습을 위해 데이터를 어떤 규칙에 따라 정렬하거나 섞는 역할\n",
    "- torchvision: 유명항 영상처리용 데이터셋, 모델, 이미지 변환기가 들어있는 패키지\n",
    "- dataset: 데이터를 읽어오는 역할\n",
    "- transforms: 불러온 이미지를 필요에 따라 변환해주는 역할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "하이퍼 파라미터 지정\n",
    "- batch_size: 배치 사이즈\n",
    "- learning_rate: Learning Rate\n",
    "- num_epoch: 반복 횟수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "learning_rate = 0.0002\n",
    "num_epoch = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dest.MNIST: MNIST Dataset을 불러온다.  \n",
    "torchvision.datasets.MNIST(root, train=True, transform=None, target_transform=None, download=False)\n",
    "- root: Root directory\n",
    "- train: train Dataset(True) or test Dataset(False)\n",
    "- donwload: Download(True), Not Dowload(False) and load by root file\n",
    "- transform: Data(PIL Image) 변형\n",
    "- target_transform: Label 변형"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9920512it [00:03, 2483025.24it/s]                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32768it [00:01, 29214.45it/s]                           \n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../MNIST/raw/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1654784it [00:00, 2107094.84it/s]                            \n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../MNIST/raw/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8192it [00:00, 13657.32it/s]            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "mnist_train = dset.MNIST(root=\"../\", train=True, transform=transforms.ToTensor(), target_transform=None, download=True)\n",
    "mnist_test = dset.MNIST(root=\"../\", train=False, transform=transforms.ToTensor(), target_transform=None, download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.utils.data import DataLoader에서 Data확인을 위하여 __getitem__과 __len__을 제공한다.  \n",
    "\n",
    "__getitem__(index):\n",
    "- Parameters: Index\n",
    "- Returns: Tuple (image, target). target is a list of captions for the image.\n",
    "- Return type: tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28]) 60000\n",
      "torch.Size([1, 28, 28]) 10000\n",
      "60000 10000\n"
     ]
    }
   ],
   "source": [
    "print(mnist_train.__getitem__(0)[0].size(), mnist_train.__len__())\n",
    "print(mnist_test.__getitem__(0)[0].size(), mnist_test.__len__())\n",
    "\n",
    "print(len(mnist_train),len(mnist_test))\n",
    "#print(mnist_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**torch.utils.data.DataLoader Parameter**  \n",
    "torch.utils.data.DataLoader(\n",
    "imagenet_data,\n",
    "batch_size=4,\n",
    "shuffle=True,\n",
    "num_workers=args.nThreads)  \n",
    "위에서 설명하였듯이 데이터를 배치로 보내기 위해서 사용하므로 batch인수 존재  \n",
    "중요한것은 Tensorflow에는 없었던 shuffle을 지원한다는 것이다.  \n",
    "Neural Network에서 마지막에 들어간 Data에 맞게 조금 더 조정되는 경향을 보이므로 Data의 순서도 생각하여 랜덤으로 섞이게 들어가게 하는 것이 조금 더 정확도를 높은 결과를 보여준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(mnist_train,batch_size=batch_size,shuffle=True,num_workers=2,drop_last=True)\n",
    "test_loader = DataLoader(mnist_test,batch_size=batch_size,shuffle=True,num_workers=2,drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "하나의 Class로서 CNN Model을 만든다.  \n",
    "**Conv2d Parameter**  \n",
    "torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros')  \n",
    "\n",
    "Tensorflow 와 다르거나 추가된 부분만 알아보자.  \n",
    "\n",
    "- in_channels: input channel의 값\n",
    "- out_channels: output의 channel의 값\n",
    "\n",
    "**여기서 중요한 것은 Tensorflow와 같이 input, output의 size를 정하는 것이 아니라 input, output의 size를 자동으로 할당하기 때문에 여기서의 channel은 image의 흑백을 의미하는 channel이 아닌 Feature의 개수라고 생각하여야 한다.**  \n",
    "\n",
    "- dilation: dilation convolution은 필터 내부에 zero padding을 추가해 강재로 receptive field를 늘리는 방법이다. 아래 그림에서 진한 파란 부분만 weight가 있고 나머지 부분은 0으로 채워진다.이러한 방법의 특징은 pooling을 수행하지도 않고도 receptive field를 크게 가져갈 수 있기 때문에 spatial dimension 손실이 적고 대부분의 weight가 0이기 때문에 연산의 효율이 좋다.\n",
    "<p>\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1200/1*SVkgHoFoiMZkjy54zM_SUw.gif\"></p>\n",
    "    \n",
    "위의 식은 아래와 같이 정의 된다.  \n",
    "$$input size = (N, C_in, H, W)$$\n",
    "$$output size = (N, C_out, H_out, W_out)$$\n",
    "\n",
    "$$out(N_i,C_{out_j}) = bias(C_{out_j}) + \\sum_{k=0}^{C_in - 1}weight(C_{out_j}, k) * input(N_i,k)$$\n",
    "\n",
    "아래 식에서 중요한 함수는 view함수가 있다.  \n",
    "nn.Linear에 값을 전달하기 위하여 Convolution의 결과의 행렬의 크기를 맞춰주어야 한다.  \n",
    "현재 DataLoader를 통하여 batch_size만큼 자동으로 전달해 주고 있으므로 (batch_size, -1)을 통하여 크기를 자동으로 맞춰주는 작업을 실시한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN,self).__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1,out_channels=16,kernel_size=5),             \n",
    "            nn.ReLU(),                                                          \n",
    "            nn.Conv2d(in_channels=16,out_channels=32,kernel_size=5),            \n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2),                               \n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5),          \n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2)    \n",
    "        )\n",
    "        self.fc_layer = nn.Sequential(                                          \n",
    "            nn.Linear(64*3*3,100),                                              \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100,10)                                                   \n",
    "        )       \n",
    "        \n",
    "    def forward(self,x):\n",
    "        out = self.layer(x)                                                     \n",
    "        out = out.view(batch_size,-1)                                            \n",
    "        out = self.fc_layer(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- LossFunction: CrossEntropy\n",
    "- Optimizer: Adam\n",
    "\n",
    "아래 에서 중요한 점은 device를 정의하는 것이다.  \n",
    "아래 Code를 통하여 GPU환경에서 돌릴수 있으면 돌리고 환경이 갖춰져 있지 않으면 CPU를 사용해서 Trainning하라는 의미이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model = CNN().to(device)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정의한 반복 횟수만큼 Model을 Trainning 하는 과정이다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.2803, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.0991, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.0606, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.0980, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.0873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.0928, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.0537, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.0246, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.0242, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "loss_arr =[]\n",
    "for i in range(num_epoch):\n",
    "    for j,[image,label] in enumerate(train_loader):\n",
    "        x = image.to(device)\n",
    "        y_= label.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model.forward(x)\n",
    "        loss = loss_func(output,y_)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if j % 1000 == 0:\n",
    "            print(loss)\n",
    "            loss_arr.append(loss.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_arr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Data Set으로 정확도 확인\n",
    "- correct: 맞은 개수를 저장할 변수\n",
    "- total: 전체 개수를 저장할 변수\n",
    "- torch.no_grad(): 기울기를 계산하지 않는 상태에서 테스트를 진행한다는 의미이다. 이런 상황에서 테스트를 진행하게 되면 계속해서 Model을 Trainning하면서 결과를 확인하게 되므로 측정하고자 하는 Model의 상태에서 Test하는 환경이 아니다.\n",
    "- torch.max(output,1): Test진행 후 Model에서 최대값과 Index를 구하는 작업\n",
    "- label.size(0): 현재 Model은 Batch로 처리하고 있기 때문에 Batch Size만큼을 반환하기 위해서 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Test Data: 98.80809783935547%\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# 인퍼런스 모드를 위해 no_grad 해줍니다.\n",
    "with torch.no_grad():\n",
    "    # 테스트로더에서 이미지와 정답을 불러옵니다.\n",
    "    for image,label in test_loader:\n",
    "        \n",
    "        # 두 데이터 모두 장치에 올립니다.\n",
    "        x = image.to(device)\n",
    "        y_= label.to(device)\n",
    "\n",
    "        # 모델에 데이터를 넣고 결과값을 얻습니다.\n",
    "        output = model.forward(x)\n",
    "        \n",
    "        # https://pytorch.org/docs/stable/torch.html?highlight=max#torch.max\n",
    "        # torch.max를 이용해 최대 값 및 최대값 인덱스를 뽑아냅니다.\n",
    "        # 여기서는 최대값은 필요없기 때문에 인덱스만 사용합니다.\n",
    "        _,output_index = torch.max(output,1)\n",
    "        \n",
    "        # 전체 개수는 라벨의 개수로 더해줍니다.\n",
    "        # 전체 개수를 알고 있음에도 이렇게 하는 이유는 batch_size, drop_last의 영향으로 몇몇 데이터가 잘릴수도 있기 때문입니다.\n",
    "        total += label.size(0)\n",
    "        \n",
    "        # 모델의 결과의 최대값 인덱스와 라벨이 일치하는 개수를 correct에 더해줍니다.\n",
    "        correct += (output_index == y_).sum().float()\n",
    "    \n",
    "    # 테스트 데이터 전체에 대해 위의 작업을 시행한 후 정확도를 구해줍니다.\n",
    "    print(\"Accuracy of Test Data: {}%\".format(100*correct/total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
